{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f14835",
   "metadata": {},
   "source": [
    "### ML PYspark\n",
    "\n",
    "The first thing to do is to create a .env file in the root of the directory. Add to the file the following two varibles \n",
    "ACCESS_KEY, ACCESS_SECRET. \n",
    "Check for more detailed explanation here: [dotenv](\"https://pypi.org/project/python-dotenv/), he explains how the .env should look like. After that, the variables are add to the os.environ and can be access as a simple dict structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum, when, input_file_name\n",
    "from functools import reduce\n",
    "import sys\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "## check\n",
    "print(f\"{os.environ['ACCESS_KEY']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94996dbf",
   "metadata": {},
   "source": [
    "## Console Login\n",
    "\n",
    "The following classes are to handle the spark on the AWS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fabe9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.s3handler import Sparker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e426f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the class\n",
    "spark = Sparker(os.environ['ACCESS_KEY'],os.environ['ACCESS_SECRET'])\n",
    "\n",
    "## local session\n",
    "spark._create_local_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c2aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the parquet and stored it \n",
    "df = spark.read_parquet(\"ubs-datasets\",\n",
    "                    \"FRACTAL/data/train/TRAIN-0436_6399-002955400.parquet\",\n",
    "                    read_all=False)\n",
    "\n",
    "# # Read the list of parquet files\n",
    "# list_s3 = [\"FRACTAL/data/train/TRAIN-1200_6136-008972557.parquet\", \"FRACTAL/data/train/TRAIN-0436_6399-002955400.parquet\"]\n",
    "# df = spark.read_parquet(\"ubs-datasets\",\n",
    "#                     list_s3,\n",
    "#                     read_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6323bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f5af63",
   "metadata": {},
   "source": [
    "see that the schema here was infered by the spark and it is totally different from when I had to download the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [col(c).isNull() for c in df.columns]\n",
    "\n",
    "##combined condition returns True for any \\\n",
    "# row where at least one column is NULL\n",
    "combined_condition = reduce(lambda a, b: a | b, conditions)\n",
    "\n",
    "print(f\"Number of cols with null values:{df.filter(combined_condition).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be364302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the array column into three separate columns\n",
    "df = df.withColumn(\"x\", col(\"xyz\")[0]) \\\n",
    "       .withColumn(\"y\", col(\"xyz\")[1]) \\\n",
    "       .withColumn(\"z\", col(\"xyz\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\",\n",
    "                        outputCol=\"scaledFeatures\",\n",
    "                        withStd=True,\n",
    "                        withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a55eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select cols\n",
    "feature_cols = ['x','y','z', 'Intensity', 'Red','Green','Blue','Infrared']  \n",
    "\n",
    "## Create an Vector Assembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "## scaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca398483",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e944d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5fda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-fractal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
