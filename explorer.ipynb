{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f14835",
   "metadata": {},
   "source": [
    "### ML PYspark\n",
    "\n",
    "The first thing to do is to create a .env file in the root of the directory. Add to the file the following two varibles \n",
    "ACCESS_KEY, ACCESS_SECRET. \n",
    "Check for more detailed explanation here: [dotenv](\"https://pypi.org/project/python-dotenv/), he explains how the .env should look like. After that, the variables are add to the os.environ and can be access as a simple dict structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94aa9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum, when, input_file_name\n",
    "from functools import reduce\n",
    "import sys\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c603316e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "## load .env\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94996dbf",
   "metadata": {},
   "source": [
    "## Console Login\n",
    "\n",
    "The following classes are to handle the spark on the AWS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fabe9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.s3handler import Sparker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4e426f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/04 05:46:14 WARN Utils: Your hostname, DESKTOP-95V5VE8, resolves to a loopback address: 127.0.1.1; using 172.30.46.218 instead (on interface eth0)\n",
      "25/11/04 05:46:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/manecomaneca/venv/spark/.venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/manecomaneca/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/manecomaneca/.ivy2.5.2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6122bced-2d9e-481e-95ed-48cb05a7c334;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 841ms :: artifacts dl 50ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6122bced-2d9e-481e-95ed-48cb05a7c334\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/22ms)\n",
      "25/11/04 05:46:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.30.46.218:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Local Session my friend</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4d9292d010>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initialize the class\n",
    "spark = Sparker(os.environ['ACCESS_KEY'],os.environ['ACCESS_SECRET'])\n",
    "\n",
    "## local session\n",
    "spark._create_local_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6cabcc",
   "metadata": {},
   "source": [
    "## Read parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a4c2aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from: ['FRACTAL/data/train/TRAIN-0436_6399-002955400.parquet']\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/mnt/d/desktop/Copernicus/classes/3-semester/bigdata/fractal/FRACTAL/data/train/TRAIN-0436_6399-002955400.parquet. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## Read the parquet and stored it \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mubs-datasets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFRACTAL/data/train/TRAIN-0436_6399-002955400.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mread_all\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# # Read the list of parquet files\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# list_s3 = [\"FRACTAL/data/train/TRAIN-1200_6136-008972557.parquet\", \"FRACTAL/data/train/TRAIN-0436_6399-002955400.parquet\"]\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# df = spark.read_parquet(\"ubs-datasets\",\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#                     list_s3,\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#                     read_all=False)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/desktop/Copernicus/classes/3-semester/bigdata/fractal/src/s3handler.py:103\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(self, bucket_name, path, read_all)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    100\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[33;03m    Stop the Spark session and release resources.\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.spark:\n\u001b[32m    104\u001b[39m         \u001b[38;5;28mself\u001b[39m.spark.stop()\n\u001b[32m    105\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSpark session stopped.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/spark/.venv/lib/python3.13/site-packages/pyspark/sql/readwriter.py:642\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    631\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    633\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    634\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    640\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/spark/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/spark/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/mnt/d/desktop/Copernicus/classes/3-semester/bigdata/fractal/FRACTAL/data/train/TRAIN-0436_6399-002955400.parquet. SQLSTATE: 42K03"
     ]
    }
   ],
   "source": [
    "## Read the parquet and stored it \n",
    "df = spark.read_parquet(\"ubs-datasets\",\n",
    "                    \"FRACTAL/data/train/TRAIN-0436_6399-002955400.parquet\",\n",
    "                    read_all=False)\n",
    "\n",
    "# # Read the list of parquet files\n",
    "# list_s3 = [\"FRACTAL/data/train/TRAIN-1200_6136-008972557.parquet\", \"FRACTAL/data/train/TRAIN-0436_6399-002955400.parquet\"]\n",
    "# df = spark.read_parquet(\"ubs-datasets\",\n",
    "#                     list_s3,\n",
    "#                     read_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6323bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f5af63",
   "metadata": {},
   "source": [
    "see that the schema here was infered by the spark and it is totally different from when I had to download the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df.groupby('classification').count()\n",
    "classes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b009f41",
   "metadata": {},
   "source": [
    "## Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b287c460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cols with null values:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "conditions = [col(c).isNull() for c in df.columns]\n",
    "\n",
    "##combined condition returns True for any \\\n",
    "# row where at least one column is NULL\n",
    "combined_condition = reduce(lambda a, b: a | b, conditions)\n",
    "\n",
    "print(f\"Number of cols with null values:{df.filter(combined_condition).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0f4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the array column into three separate columns\n",
    "df = df.withColumn(\"x\", col(\"xyz\")[0]) \\\n",
    "       .withColumn(\"y\", col(\"xyz\")[1]) \\\n",
    "       .withColumn(\"z\", col(\"xyz\")[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e86b2a",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5baa6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.s3handler import FeatureEngineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa096956",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureEngineering = FeatureEngineering(df)\n",
    "df = featureEngineering.apply_all()\n",
    "# it can be used many feature engineering such as\n",
    "# height_above_ground(self, grid_size=5.0)\n",
    "# local_stats(self, grid_size=2.0)\n",
    "# return_features(self)\n",
    "# vegetation_index(self)\n",
    "# water_detection(self)\n",
    "# or applying all with apply_all(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad4a714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------+---------------+-----------------+----------------+--------------+---------+--------+--------+-------+-------------+--------+-------------+-------------------+-----------+-----+-----+-----+--------+--------------------+----------+-----------+------+--------------------+-------------+-------------------+-----------------+-------------------+------------+----------------+--------------+-------------------+------------------+------------------+\n",
      "|                 xyz|Intensity|ReturnNumber|NumberOfReturns|ScanDirectionFlag|EdgeOfFlightLine|Classification|Synthetic|KeyPoint|Withheld|Overlap|ScanAngleRank|UserData|PointSourceId|            GpsTime|ScanChannel|  Red|Green| Blue|Infrared|                 wkb|         x|          y|     z| height_above_ground|local_density|        local_z_std|    local_z_range|          roughness|return_ratio|is_single_return|is_last_return|               ndvi|   green_red_ratio|              ndwi|\n",
      "+--------------------+---------+------------+---------------+-----------------+----------------+--------------+---------+--------+--------+-------+-------------+--------+-------------+-------------------+-----------+-----+-----+-----+--------+--------------------+----------+-----------+------+--------------------+-------------+-------------------+-----------------+-------------------+------------+----------------+--------------+-------------------+------------------+------------------+\n",
      "|[436950.005, 6398...|     2744|           1|              1|                1|               0|             2|        0|       0|       0|      0|       10.998|       0|           46|3.073370757053636E8|          0|24064|23296|27136|    5120|[01 E9 03 00 00 5...|436950.005| 6398458.04|14.301|0.045999999999999375|          113|0.03123873248196348|0.136000000000001|0.21396392110933743|         1.0|               1|             1|-0.6491227847751239|0.9680850661533799|0.6396396171297996|\n",
      "|[436950.303, 6398...|     1771|           1|              1|                1|               0|             2|        0|       0|       0|      0|       10.998|       0|           46|3.073370757120294E8|          0|24064|23296|27136|    6144|[01 E9 03 00 00 C...|436950.303|6398458.129|14.337| 0.08199999999999896|          113|0.03123873248196348|0.136000000000001|0.21396392110933743|         1.0|               1|             1| -0.593220319345196|0.9680850661533799| 0.582608675862477|\n",
      "|[436950.752, 6398...|     1893|           1|              1|                1|               0|             2|        0|       0|       0|      0|         12.0|       0|           46|3.073370757186948E8|          0|19456|18432|21248|    5632|[01 E9 03 00 00 B...|436950.752|6398458.165|14.311| 0.05599999999999916|          113|0.03123873248196348|0.136000000000001|0.21396392110933743|         1.0|               1|             1|-0.5510203861997613| 0.947368372359767|0.5319148715128461|\n",
      "+--------------------+---------+------------+---------------+-----------------+----------------+--------------+---------+--------+--------+-------+-------------+--------+-------------+-------------------+-----------+-----+-----+-----+--------+--------------------+----------+-----------+------+--------------------+-------------+-------------------+-----------------+-------------------+------------+----------------+--------------+-------------------+------------------+------------------+\n",
      "only showing top 3 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d481843",
   "metadata": {},
   "source": [
    "## Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be364302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\",\n",
    "                        outputCol=\"scaledFeatures\",\n",
    "                        withStd=True,\n",
    "                        withMean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0959046",
   "metadata": {},
   "source": [
    "## Choosing correct columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a55eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select cols\n",
    "feature_cols = ['x','y','z', 'Intensity', 'Red','Green','Blue','Infrared']  \n",
    "\n",
    "## Feature Enginering\n",
    "\n",
    "## Create an Vector Assembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "## scaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca398483",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e944d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5fda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
