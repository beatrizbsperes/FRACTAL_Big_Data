{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f14835",
   "metadata": {},
   "source": [
    "### ML PYspark\n",
    "\n",
    "The first thing to do is to create a .env file in the root of the directory. Add to the file the following two varibles \n",
    "ACCESS_KEY, ACCESS_SECRET. \n",
    "Check for more detailed explanation here: [dotenv](\"https://pypi.org/project/python-dotenv/), he explains how the .env should look like. After that, the variables are add to the os.environ and can be access as a simple dict structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94aa9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum, when, input_file_name\n",
    "from functools import reduce\n",
    "import sys\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c603316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AKIATL5DQEXAENZHWCKT\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "## check\n",
    "print(f\"{os.environ['ACCESS_KEY']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94996dbf",
   "metadata": {},
   "source": [
    "## Console Login\n",
    "\n",
    "The following classes are to handle the spark on the AWS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fabe9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.s3handler import Sparker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4e426f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/30 13:26:02 WARN Utils: Your hostname, DESKTOP-95V5VE8, resolves to a loopback address: 127.0.1.1; using 172.30.46.218 instead (on interface eth0)\n",
      "25/10/30 13:26:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/manecomaneca/venv/spark/.venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/manecomaneca/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/manecomaneca/.ivy2.5.2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6692e31b-c8b2-4e84-8263-b9416c14cd3f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 772ms :: artifacts dl 36ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6692e31b-c8b2-4e84-8263-b9416c14cd3f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/19ms)\n",
      "25/10/30 13:26:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.30.46.218:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Local Session my friend</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6cdd421160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initialize the class\n",
    "spark = Sparker(os.environ['ACCESS_KEY'],os.environ['ACCESS_SECRET'])\n",
    "\n",
    "## local session\n",
    "spark._create_local_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a4c2aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from: s3a://ubs-datasets/FRACTAL/data/train/TRAIN-1200_6136-008972557.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Read the parquet and stored it \n",
    "df = spark.read_parquet(\"ubs-datasets\",\n",
    "                    \"FRACTAL/data/train/TRAIN-1200_6136-008972557.parquet\",\n",
    "                    read_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d6323bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- xyz: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- Intensity: integer (nullable = true)\n",
      " |-- ReturnNumber: short (nullable = true)\n",
      " |-- NumberOfReturns: short (nullable = true)\n",
      " |-- ScanDirectionFlag: short (nullable = true)\n",
      " |-- EdgeOfFlightLine: short (nullable = true)\n",
      " |-- Classification: short (nullable = true)\n",
      " |-- Synthetic: short (nullable = true)\n",
      " |-- KeyPoint: short (nullable = true)\n",
      " |-- Withheld: short (nullable = true)\n",
      " |-- Overlap: short (nullable = true)\n",
      " |-- ScanAngleRank: float (nullable = true)\n",
      " |-- UserData: short (nullable = true)\n",
      " |-- PointSourceId: integer (nullable = true)\n",
      " |-- GpsTime: double (nullable = true)\n",
      " |-- ScanChannel: short (nullable = true)\n",
      " |-- Red: integer (nullable = true)\n",
      " |-- Green: integer (nullable = true)\n",
      " |-- Blue: integer (nullable = true)\n",
      " |-- Infrared: integer (nullable = true)\n",
      " |-- wkb: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f5af63",
   "metadata": {},
   "source": [
    "see that the schema here was infered by the spark and it is totally different from when I had to download the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fdd08cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(xyz=[1200800.1400000001, 6135380.53, 1171.64], Intensity=342, ReturnNumber=2, NumberOfReturns=3, ScanDirectionFlag=1, EdgeOfFlightLine=0, Classification=5, Synthetic=0, KeyPoint=0, Withheld=0, Overlap=0, ScanAngleRank=16.00200080871582, UserData=0, PointSourceId=3125, GpsTime=304320466.8181704, ScanChannel=0, Red=11264, Green=15360, Blue=15872, Infrared=40192, wkb=bytearray(b'\\x01\\xe9\\x03\\x00\\x00>\\n\\xd7#\\xa0R2A\\x1f\\x85\\xeb!\\x95gWA\\xc3\\xf5(\\\\\\x8fN\\x92@'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd8584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 209895\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287c460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions = [col(c).isNull() for c in df.columns]\n",
    "\n",
    "##combined condition returns True for any \\\n",
    "# row where at least one column is NULL\n",
    "combined_condition = reduce(lambda a, b: a | b, conditions)\n",
    "\n",
    "print(f\"Number of cols with null values:{df.filter(combined_condition).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be364302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba0f4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the array column into three separate columns\n",
    "df = df.withColumn(\"x\", col(\"xyz\")[0]) \\\n",
    "       .withColumn(\"y\", col(\"xyz\")[1]) \\\n",
    "       .withColumn(\"z\", col(\"xyz\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53db8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\",\n",
    "                        outputCol=\"scaledFeatures\",\n",
    "                        withStd=True,\n",
    "                        withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3a55eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select cols\n",
    "feature_cols = ['x','y','z', 'Intensity', 'Red','Green','Blue','Infrared']  \n",
    "\n",
    "## Create an Vector Assembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "## scaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca398483",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e944d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+\n",
      "|features                                                                        |\n",
      "+--------------------------------------------------------------------------------+\n",
      "|[1200800.1400000001,6135380.53,1171.64,342.0,11264.0,15360.0,15872.0,40192.0]   |\n",
      "|[1200800.19,6135380.37,1171.34,301.0,11776.0,16128.0,16384.0,39168.0]           |\n",
      "|[1200800.27,6135379.54,1171.03,259.0,9216.0,12800.0,12288.0,27648.0]            |\n",
      "|[1200800.06,6135381.44,1164.2,317.0,9984.0,13568.0,14336.0,26624.0]             |\n",
      "|[1200800.45,6135381.19,1166.57,281.0,9472.0,12800.0,13312.0,32000.0]            |\n",
      "|[1200800.24,6135381.48,1164.3600000000001,826.0,9984.0,13568.0,14336.0,28416.0] |\n",
      "|[1200801.3,6135379.9,1170.14,328.0,9472.0,12800.0,11776.0,27136.0]              |\n",
      "|[1200801.43,6135379.59,1170.25,434.0,9472.0,12800.0,11776.0,24576.0]            |\n",
      "|[1200801.53,6135379.33,1170.3,387.0,8704.0,11776.0,11008.0,25856.0]             |\n",
      "|[1200800.08,6135382.72,1163.0,1232.0,8960.0,12032.0,11008.0,29952.0]            |\n",
      "|[1200800.18,6135382.48,1163.04,1275.0,8960.0,12032.0,11008.0,24576.0]           |\n",
      "|[1200800.42,6135381.99,1163.96,252.0,9984.0,13568.0,14336.0,26624.0]            |\n",
      "|[1200800.29,6135382.21,1163.07,565.0,8192.0,11520.0,11008.0,23296.0]            |\n",
      "|[1200801.21,6135380.55,1168.6200000000001,367.0,13824.0,18176.0,19200.0,32768.0]|\n",
      "|[1200800.9000000001,6135381.08,1166.46,249.0,12544.0,16640.0,18432.0,28416.0]   |\n",
      "|[1200801.53,6135379.92,1170.06,422.0,9472.0,12800.0,11776.0,27904.0]            |\n",
      "|[1200801.6500000001,6135379.62,1170.24,727.0,13056.0,17152.0,16128.0,33024.0]   |\n",
      "|[1200801.75,6135379.39,1170.28,860.0,11264.0,14848.0,13568.0,28672.0]           |\n",
      "|[1200800.06,6135382.19,1163.08,959.0,8192.0,11520.0,11008.0,23808.0]            |\n",
      "|[1200800.6400000001,6135381.09,1166.33,446.0,12544.0,16640.0,18432.0,32768.0]   |\n",
      "+--------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5fda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
